{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da88023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.transforms as T\n",
    "from torch.distributions import Transform, TransformedDistribution, MultivariateNormal, Normal\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "import scipy.io as sio\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deef28ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarized_IQR(data):\n",
    "    standardized_IQR_data = torch.zeros(data.shape)\n",
    "    IQR = torch.zeros(data.shape[1])\n",
    "    median = torch.zeros(data.shape[1])\n",
    "\n",
    "    for i in range(data.shape[1]):\n",
    "        column = data[:, i]\n",
    "\n",
    "        median[i] = torch.median(column)\n",
    "\n",
    "        q1 = torch.quantile(column, 0.25)\n",
    "        q3 = torch.quantile(column, 0.75)\n",
    "\n",
    "        IQR[i] = q3 - q1\n",
    "\n",
    "        standardized_column = (column - median[i]) / IQR[i]\n",
    "\n",
    "        standardized_IQR_data[:, i] = standardized_column\n",
    "\n",
    "    return standardized_IQR_data.cuda(), median.cuda(), IQR.cuda()\n",
    "\n",
    "def inv_standarized_IQR(data, medians, iqrs):\n",
    "    original_data = torch.zeros(data.shape)\n",
    "\n",
    "    for i in range(data.shape[1]):\n",
    "        standardized_column = data[:, i]\n",
    "        median = medians[i]\n",
    "        iqr = iqrs[i]\n",
    "\n",
    "        original_column = (standardized_column * iqr) + median\n",
    "        original_data[:, i] = original_column\n",
    "\n",
    "    return original_data.cuda()\n",
    "\n",
    "\n",
    "def logit_transform(data, lower_bounds, upper_bounds):\n",
    "    data, lower_bounds, upper_bounds = data.cpu(), lower_bounds.cpu(), upper_bounds.cpu()\n",
    "    n, nvar = data.shape\n",
    "    trans_data = torch.zeros([n, nvar])\n",
    "\n",
    "    for i in range(n):\n",
    "        num = data[i, :] - lower_bounds\n",
    "        denom = upper_bounds - data[i, :]\n",
    "        trans_data[i, :] = torch.log(num / denom)\n",
    "\n",
    "    return trans_data.cuda()\n",
    "\n",
    "def inverse_logit_transform(data, lower_bounds, upper_bounds):\n",
    "    data, lower_bounds, upper_bounds = data.cpu(), lower_bounds.cpu(), upper_bounds.cpu()\n",
    "    n, nvar = data.shape\n",
    "    trans_inv_data = torch.zeros([n, nvar])\n",
    "\n",
    "    for i in range(n):\n",
    "        num = torch.exp(data[i, :]) + lower_bounds\n",
    "        denom = 1 + torch.exp(data[i, :])\n",
    "        trans_inv_data[i, :] = num / denom\n",
    "\n",
    "    return trans_inv_data.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf14667",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 6 # number of parameters\n",
    "\n",
    "# get the lower bound and upper bound for logit transform\n",
    "lower_bounds = torch.ones(n+1).cuda()\n",
    "lower_bounds[0:n] = -1.0\n",
    "lower_bounds[n] = 0.0\n",
    "upper_bounds = torch.ones(n+1).cuda()\n",
    "\n",
    "# get the data\n",
    "full_dataset = sio.loadmat(\"/content/drive/MyDrive/Colab Notebooks/SVAR project/data_GVAR_dim6.mat\") # the dataset contain pairs, obs and 'true' theta\n",
    "#simulated_dataset = sio.loadmat(\"/content/drive/MyDrive/Colab Notebooks/SVAR project/SNPE_prior_dimi6.mat\") # dataset contian sims and its corresponding theta\n",
    "\n",
    "# get the prior from smc pilot run\n",
    "theta_smc = sio.loadmat(\"/content/drive/MyDrive/Colab Notebooks/SVAR project/results_summ_pilot_dim6.mat\")['part_vals_smc']\n",
    "x_smc = sio.loadmat(\"/content/drive/MyDrive/Colab Notebooks/SVAR project/results_summ_pilot_dim6.mat\")['part_sim_smc']\n",
    "\n",
    "# SMC-NPE reuslt\n",
    "smc_npe_result = sio.loadmat(\"/content/drive/MyDrive/Colab Notebooks/SVAR project/SNPE_smc_dim6.mat\")['theta_smc_dim6_NPE']\n",
    "\n",
    "theta_true = full_dataset['theta_true']\n",
    "#theta = torch.from_numpy(simulated_dataset['part_vals']).to(torch.float32).cuda()\n",
    "#x = torch.from_numpy(simulated_dataset['part_sim']).to(torch.float32).cuda()\n",
    "theta = torch.from_numpy(theta_smc).to(torch.float32).cuda()\n",
    "x = torch.from_numpy(x_smc).to(torch.float32).cuda()\n",
    "x_0 = torch.from_numpy(full_dataset['sy']).to(torch.float32).cuda()\n",
    "\n",
    "generate_model = model(n+1, full_dataset, 1000)\n",
    "\n",
    "for i in range(theta.shape[0]):\n",
    "    for j in range(theta.shape[1]):\n",
    "        if x[i, j] > 1e30:\n",
    "            theta[i, :] = dist.Uniform(lower_bounds, upper_bounds).sample().cuda()\n",
    "            x[i, :] = generate_model.run_simulation(theta[i, :]).cuda()\n",
    "\n",
    "    while torch.isinf(x[i,:]).any() == True:\n",
    "        theta[i, :] = dist.Uniform(lower_bounds, upper_bounds).sample().cuda()\n",
    "        x[i, :] = generate_model.run_simulation(theta[i, :]).cuda()\n",
    "\n",
    "print(torch.isinf(x).any())\n",
    "print(torch.isnan(x).any())\n",
    "\n",
    "# transform the theta\n",
    "theta_logit = logit_transform(theta, lower_bounds, upper_bounds)\n",
    "theta_trans, theta_median, theta_iqrs = standarized_IQR(theta_logit)\n",
    "\n",
    "# transform the simulated data\n",
    "x_trans, x_median, x_iqrs = standarized_IQR(x)\n",
    "\n",
    "# transform the observation data\n",
    "y_trans = (x_0 - x_median)/x_iqrs\n",
    "\n",
    "# check the cuda\n",
    "print(theta_logit.shape, theta_trans.shape, x_trans.shape, y_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8839c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPE:\n",
    "    def __init__(self, x_trans, theta_trans, n, steps, lr, batch_size):\n",
    "        # Dataset preparation\n",
    "        self.full_dataset = TensorDataset(x_trans, theta_trans)\n",
    "        n_total = len(self.full_dataset)\n",
    "        n_train = int(n_total * 0.7)\n",
    "        n_val = n_total - n_train\n",
    "        self.train_dataset, self.val_dataset = random_split(self.full_dataset, [n_train, n_val])\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.val_loader = DataLoader(self.val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Initialize distributions and modules\n",
    "        self.dist_base = Normal(torch.zeros(n+1).cuda(), torch.ones(n+1).cuda())\n",
    "        self.theta_transform = T.conditional_spline(n+1, context_dim=n+1)\n",
    "        self.dist_theta_given_x = dist.ConditionalTransformedDistribution(self.dist_base, [self.theta_transform])\n",
    "        self.modules = torch.nn.ModuleList([self.theta_transform]).cuda()\n",
    "        self.optimizer = torch.optim.Adam(self.modules.parameters(), lr=lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1000, gamma=0.9)  # New\n",
    "\n",
    "        self.steps = steps\n",
    "        self.progress = ProgressBar(self.steps, fmt=ProgressBar.FULL)\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def train_one_step(self, x_batch, theta_batch):\n",
    "        self.optimizer.zero_grad()\n",
    "        ln_p_theta_given_x = self.dist_theta_given_x.condition(x_batch.detach()).log_prob(theta_batch.detach())\n",
    "        train_loss = -ln_p_theta_given_x.sum()\n",
    "        train_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.dist_theta_given_x.clear_cache()\n",
    "        return train_loss.item()\n",
    "\n",
    "    def validate(self):\n",
    "        val_loss = 0.0\n",
    "        for batch_idx, (x_val_batch, theta_val_batch) in enumerate(self.val_loader):\n",
    "            ln_p_theta_given_x_val = self.dist_theta_given_x.condition(x_val_batch.detach()).log_prob(theta_val_batch.detach())\n",
    "            val_loss += -ln_p_theta_given_x_val.sum().item()\n",
    "        val_loss /= len(self.val_loader)\n",
    "        return val_loss\n",
    "\n",
    "    def train(self):\n",
    "        for step in range(self.steps):\n",
    "            for batch_idx, (x_batch, theta_batch) in enumerate(self.train_loader):\n",
    "                train_loss = self.train_one_step(x_batch, theta_batch)\n",
    "            self.train_losses.append(train_loss)\n",
    "\n",
    "            val_loss = self.validate()\n",
    "            self.val_losses.append(val_loss)\n",
    "\n",
    "            self.scheduler.step()  # New\n",
    "\n",
    "            if step % 200 == 0:\n",
    "                print(f'step: {step}, train_loss: {train_loss}, val_loss: {val_loss}')\n",
    "\n",
    "            self.progress.current += 1\n",
    "            self.progress()\n",
    "\n",
    "        print('finish training')\n",
    "        self.progress.done()\n",
    "\n",
    "        # Return the final trained distribution\n",
    "        return self.dist_theta_given_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd330060",
   "metadata": {},
   "outputs": [],
   "source": [
    "NPE_iter1 = NPE(x_trans, theta_trans, n, steps=801, lr=1e-4, batch_size=64)\n",
    "NPE_model1 = NPE_iter1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc4c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(NPE_iter1.train_losses, label='Train Loss')\n",
    "plt.plot(NPE_iter1.val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707795c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_flow = NPE_model1.condition(y_trans).sample(torch.Size([1000,]))\n",
    "posterior = inverse_logit_transform(inv_standarized_IQR(theta_flow, theta_median, theta_iqrs), lower_bounds, upper_bounds)\n",
    "\n",
    "figure(figsize=(20, 10))\n",
    "for i in range(1,8):\n",
    "    plt.subplot(2,4,i)\n",
    "    sns.kdeplot(data = posterior_prior[:, i-1].cpu())\n",
    "    sns.kdeplot(data = posterior[:, i-1].cpu())\n",
    "    #sns.kdeplot(data = bsl_pos[:, i-1])\n",
    "    plt.axvline(theta_true[i-1])\n",
    "    plt.legend(labels=['Uniform prior NPE','SMC-prior NPE','bsl', 'True'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
