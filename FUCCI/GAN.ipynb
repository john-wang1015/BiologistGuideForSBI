{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c81d751d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aufb\\AppData\\Local\\Temp\\ipykernel_13140\\2404262712.py:26: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('png', 'pdf')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "\n",
    "from idlmam import train_network, Flatten, weight_reset, View, set_seed\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png', 'pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2886d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic=True\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87d040b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b5b5258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleGAN(latent_d, neurons, out_shape, sigmoidG=False, leak=0.2):\n",
    "    \"\"\"\n",
    "    This function will create a simple GAN for us to train. It will return a \n",
    "    tuple (G, D), holding the generator and discriminator network respectively. \n",
    "    \n",
    "    latent_d: the number of latent variables we will use as input to the generator G. \n",
    "    neurons: how many hidden neurons to use in each hidden layer\n",
    "    out_shape: the shape of the output of the discriminator D. This should be the \n",
    "    same shape as the real data. \n",
    "    sigmoidG: true if the generator G should end with a sigmoid activation, or \n",
    "    False if it should just return unbounded activations\n",
    "    \"\"\"\n",
    "    G = nn.Sequential(\n",
    "        fcLayer(latent_d, neurons, leak),\n",
    "        fcLayer(neurons, neurons, leak),\n",
    "        fcLayer(neurons, neurons, leak),\n",
    "        nn.Linear(neurons, abs(np.prod(out_shape)) ),# np.prod will multiply each value together in the shape, giving us the total number of needed outputs. abs will remove the impact of \"-1\" for the batch dimension.\n",
    "        View(out_shape)#Now re-shape the output to whatever D expects. \n",
    "    )\n",
    "    #Sometimes we do/don't want G to return a sigmoid value (i.e., [0,1]), so we wrap it in an a conditional\n",
    "    if sigmoidG:\n",
    "        G = nn.Sequential(G, nn.Sigmoid())\n",
    "    \n",
    "    D = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        fcLayer(abs(np.prod(out_shape)), neurons, leak),\n",
    "        fcLayer(neurons, neurons, leak),\n",
    "        fcLayer(neurons, neurons, leak),\n",
    "        nn.Linear(neurons, 1 ) #D has 1 output for a binary classification problem\n",
    "    )\n",
    "    return G, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc246fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalWrapper(nn.Module):\n",
    "    def __init__(self, input_shape, neurons, classes, main_network, leak=0.2):\n",
    "        \"\"\" \n",
    "        input_shape: the shape that the latent variable $\\boldsymbol{z}$ should take.\n",
    "        neurons: nuerons to use in hidden layers\n",
    "        classes: number of classes in labels $y$\n",
    "        main_network: either the generator $G$ or discriminator $D$\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.classes = classes\n",
    "        #figure out number of latent parameters from the latent shape\n",
    "        input_size = abs(np.prod(input_shape))\n",
    "        #create an embedding layer to convert labels to vectors\n",
    "        self.label_embedding = nn.Embedding(classes, input_size)\n",
    "        \n",
    "        #In the forward function we will concatenate the label and original date into one vector. Then this ‘combiner’ will take that \n",
    "        #extra large tensor and create a new tensor that is the size of just the original ‘input_shape’. This does the work of merging \n",
    "        #the conditional information (from label_embedding) into the latent vector.\n",
    "        self.combiner = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            fcLayer(input_size*2, input_size, leak=leak),#one FC layer\n",
    "            nn.Linear(input_size, input_size),#A second FC layer, but first linear & activtion applied\n",
    "            nn.LeakyReLU(leak),\n",
    "            View(input_shape), #So that we can re-shape the output and apply normalizing based on the target output shape. This makes the Conditional wrapper useful for linear and convolutional models. \n",
    "            nn.LayerNorm(input_shape[1:]),\n",
    "        )\n",
    "        self.net = main_network\n",
    "        \n",
    "    \n",
    "    #The forward function is the code that takes an input and produce an output. \n",
    "    def forward(self, x, condition=None):\n",
    "        if condition is None:#if no label was given, lets pick one at random\n",
    "            condition = torch.randint(0, self.classes, size=(x.size(0),), device=x.get_device()) \n",
    "        #embed the label and re-shape it as desied\n",
    "        embd = self.label_embedding(condition)\n",
    "        #make sure the label embd and data x have the same shape so that we can concatenate them\n",
    "        embd = embd.view(self.input_shape)\n",
    "        x = x.view(self.input_shape)\n",
    "        #concatenate the latent input with the embedded label\n",
    "        x_comb = torch.cat([x, embd], dim=1)\n",
    "        #return the result of the network on the combined inputs\n",
    "        return self.net(self.combiner(x_comb))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebb052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "latent_d = 128\n",
    "neurons = 512\n",
    "out_shape = (-1, 28, 28) #You could also do (-1, 1, 28, 28) for 1 channel, but that makes numpy code a little more cumbersome later\n",
    "num_epochs = 10\n",
    "\n",
    "def fcLayer(in_neurons, out_neurons, leak=0.1): #our helper function\n",
    "    \"\"\"\n",
    "    in_neurons: how many inputs to this layer\n",
    "    out_neurons: how many outputs for this layer\n",
    "    leak: the leaky relu leak value. \n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_neurons, out_neurons),\n",
    "        nn.LeakyReLU(leak),\n",
    "        nn.LayerNorm(out_neurons)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890ec3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_d = 128\n",
    "out_shape = (-1, 1, 28, 28)\n",
    "in_shape = (-1, latent_d)\n",
    "classes = 10\n",
    "G, D = simpleGAN(latent_d, neurons, out_shape, sigmoidG=True)\n",
    "\n",
    "G = ConditionalWrapper(in_shape, neurons, classes, G)\n",
    "D = ConditionalWrapper(out_shape, neurons, classes, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78bb1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_c_wgan(D, G, loader, latent_d, epochs=20, device=\"gpu\"):\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "\n",
    "    G = G.to(device)\n",
    "    D = D.to(device)\n",
    "\n",
    "    # Setup Adam optimizers for both G and D\n",
    "    optimizerD = torch.optim.AdamW(D.parameters(), lr=0.0001, betas=(0.0, 0.9))\n",
    "    optimizerG = torch.optim.AdamW(G.parameters(), lr=0.0001, betas=(0.0, 0.9))\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for data in tqdm(loader, leave=False):\n",
    "            if isinstance(data, tuple) or len(data) == 2:\n",
    "                data, class_label = data\n",
    "            batch_size = data.size(0)\n",
    "            D.zero_grad()\n",
    "            G.zero_grad()\n",
    "            real = data.to(device)\n",
    "            class_label = class_label.to(device)\n",
    "            # Step 1) D-score, G-score, and gradient penalty\n",
    "            #How well does D work on real data \n",
    "            D_success = D(real, class_label)\n",
    "\n",
    "            ## Train with all-fake batch\n",
    "            # Generate batch of latent vectors\n",
    "            noise = torch.randn(batch_size, latent_d, device=device)\n",
    "            # Generate fake image batch with G\n",
    "            fake = G(noise, class_label) \n",
    "            # Classify all fake batch with D\n",
    "            D_failure = D(fake, class_label) \n",
    "\n",
    "            #Now calculate for gradient penalty\n",
    "            eps_shape = [batch_size]+[1]*(len(data.shape)-1)\n",
    "            eps = torch.rand(eps_shape, device=device)\n",
    "            fake = eps*real + (1-eps)*fake\n",
    "            output = D(fake, class_label) \n",
    "\n",
    "            grad = torch.autograd.grad(outputs=output, inputs=fake,\n",
    "                                  grad_outputs=torch.ones(output.size(), device=device),\n",
    "                                  create_graph=True, retain_graph=True, only_inputs=True, allow_unused=True)[0]\n",
    "\n",
    "            D_grad_penalty = ((grad.norm(2, dim=1) - 1) ** 2).mean() \n",
    "\n",
    "            # Calculate D's loss on the all-fake batch\n",
    "            errD = (D_failure-D_success).mean() + D_grad_penalty.mean()*10\n",
    "            errD.backward()\n",
    "            # Update D\n",
    "            optimizerD.step()\n",
    "            \n",
    "            D_losses.append(errD.item())\n",
    "\n",
    "            # Step 2) -D(G(z))\n",
    "            D.zero_grad()\n",
    "            G.zero_grad()\n",
    "            # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "\n",
    "            noise = torch.randn(batch_size, latent_d, device=device)\n",
    "            output = -D(G(noise, class_label), class_label)\n",
    "            # Calculate G's loss based on this output\n",
    "            errG = output.mean()\n",
    "            # Calculate gradients for G\n",
    "            errG.backward()\n",
    "            # Update G\n",
    "            optimizerG.step()\n",
    "            \n",
    "            G_losses.append(errG.item())\n",
    "            \n",
    "    return D_losses, G_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90041521",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.MNIST(\"./\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = torchvision.datasets.MNIST(\"./\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf92ea0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D_losses, G_losses = train_c_wgan(D, G, train_loader, latent_d, epochs=20, device=device)\n",
    "\n",
    "G = G.eval()\n",
    "D = D.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea21251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCEWithLogitsLoss function. The BCE loss is for binary classification problems, which ours is (real vs fake)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = torch.optim.AdamW(D.parameters(), lr=0.0001, betas=(0.0, 0.9))\n",
    "optimizerG = torch.optim.AdamW(G.parameters(), lr=0.0001, betas=(0.0, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d7309c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for data, class_label in tqdm(train_loader, leave=False):\n",
    "        # prep batch & make labels\n",
    "        real_data = data.to(device)\n",
    "        y_real = torch.full((batch_size,1), real_label, dtype=torch.float32, device=device)\n",
    "        y_fake = torch.full((batch_size,1), fake_label, dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Step 1) $\\ell ( D( x_{\\mathit{real}}) ,\\ y_{\\mathit{real}})\\$ and $\\ell ( D(\\boldsymbol{x}_{\\mathit{fake}}) ,\\ y_{\\mathit{fake}})$\n",
    "        D.zero_grad()\n",
    "\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = loss_func(D(real_data), y_real)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors $z \\sim \\mathcal{N}(\\vec{0}, 1)$\n",
    "        z = torch.randn(batch_size, latent_d, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        # Classify all fake batch with D. We will save this to re-use for the 2nd step. \n",
    "        fake = G(z) \n",
    "        #Why do we detach here? Because we don't want the gradient to impact G. \n",
    "        #Our goal right now is to update _just_ the discriminator. \n",
    "        #BUT, we will re-use this fake data for updating the discriminator, so we want to save the \n",
    "        #non-detached version! \n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = loss_func(D(fake.detach()), y_fake)\n",
    "        # Calculate the gradients for this batch\n",
    "        errD_fake.backward()\n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Step 2) $\\ell ( D(\\boldsymbol{x}_{\\mathit{fake}}) ,\\ y_{\\mathit{real}})$\n",
    "        G.zero_grad()\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = loss_func(D(fake), y_real)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "        \n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e35dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    noise = torch.randn(batch_size, latent_d, device=device) #$\\boldsymbol{z} \\sim \\mathcal{N}(\\vec{0}, \\boldsymbol{I})$\n",
    "    fake_digits = G(noise) \n",
    "    scores = torch.sigmoid(D(fake_digits))\n",
    "    \n",
    "    fake_digits = fake_digits.cpu()\n",
    "    scores = scores.cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eafc6af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_gen_imgs(fake_digits, scores=None):\n",
    "    batch_size = fake_digits.size(0)\n",
    "    #This code assumes we are working with black-and-white images\n",
    "    fake_digits = fake_digits.reshape(-1, fake_digits.size(-1), fake_digits.size(-1))\n",
    "    i_max = int(round(np.sqrt(batch_size)))\n",
    "    j_max = int(np.floor(batch_size/float(i_max)))\n",
    "    f, axarr = plt.subplots(i_max,j_max, figsize=(10,10))\n",
    "    for i in range(i_max):\n",
    "        for j in range(j_max):\n",
    "            indx = i*j_max+j\n",
    "            axarr[i,j].imshow(fake_digits[indx,:].numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "            axarr[i,j].set_axis_off()\n",
    "            if scores is not None:\n",
    "                axarr[i,j].text(0.0, 0.5, str(round(scores[indx],2)), dict(size=20, color='red'))\n",
    "plot_gen_imgs(fake_digits, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fb6953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74d6eb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    #Generate 10 latent noise vectors, and repeat them 10 times. So we are re-using the same latent codes\n",
    "    noise =  torch.randn(10, latent_d, device=device).repeat((1,10)).view(-1, latent_d)\n",
    "    #Count from 0 to 9, then wrap around back to 0 again. This is done 10 times\n",
    "    labels = torch.fmod(torch.arange(0, noise.size(0), device=device), classes)\n",
    "    #Now we have the same latent in noise being used to generate 10 images, but changing the label each time. \n",
    "    fake_digits = G(noise, labels)\n",
    "    scores = D(fake_digits, labels)\n",
    "    \n",
    "    fake_digits = fake_digits.cpu()\n",
    "    scores = scores.cpu().numpy().flatten()\n",
    "plot_gen_imgs(fake_digits)#When we plot the results, we should see a grid of digits going from 0 to 9, where each row all used the same latent vector and share similar visual properties.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0cdbf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
